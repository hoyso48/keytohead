{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.13.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.15.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorflow-addons\n",
    "import tensorflow_addons as tfa\n",
    "# !pip install datasets\n",
    "# from datasets import *\n",
    "# from datasets import load_dataset\n",
    "\n",
    "!pip install -q -U tensorflow-text\n",
    "import tensorflow_text as text\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.125.75.114:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.125.75.114:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU: grpc://10.125.75.114:8470\n",
      "Running on 8 replicas\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def auto_select_accelerator():\n",
    "    TPU_DETECTED = False\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.TPUStrategy(tpu)\n",
    "        print(\"Running on TPU:\", tpu.master())\n",
    "        TPU_DETECTED = True\n",
    "    except ValueError:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "\n",
    "    return strategy, TPU_DETECTED\n",
    "tf.keras.backend.clear_session()\n",
    "strategy, tpu_detected = auto_select_accelerator()\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd gig\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.layers as L\n",
    "from sklearn.model_selection import train_test_split\n",
    "base_dir = '/content/drive/MyDrive/Colab Notebooks/keytohead/'\n",
    "data_dir = base_dir + 'data/'\n",
    "SAVE_DIR = base_dir + 'models/'\n",
    "GCS_PATH = 'gs://stylex/'\n",
    "\n",
    "from utils import *\n",
    "from models import *\n",
    "from dataloader import *\n",
    "from tokenizer_configs import tokenizer, bert_tokenizer, random_selector, _VOCAB, _PLAIN_TOKEN, _HUMOR_TOKEN, _ROMANCE_TOKEN, _CLICK_TOKEN, _MASK_TOKEN\n",
    "from configs import *\n",
    "from layers import *\n",
    "from translator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickbait = pd.read_csv(data_dir+'clickbait/raw/clickbait.raw', header = None, names = ['text'])\n",
    "humor = pd.read_csv(data_dir+'humor/raw/humor.raw', sep = '이런건없겠지', header = None, names = ['text'], engine='python')\n",
    "romance = pd.read_csv(data_dir+'romance/raw/romance.raw', sep = '이런건없겠지', header = None, names = ['text'], engine='python')\n",
    "headlines_train = pd.read_csv(data_dir+'CNN_NYT/raw/headlines.train', sep = '이런건없겠지', header = None, names = ['text'], engine='python')\n",
    "headlines_valid = pd.read_csv(data_dir+'CNN_NYT/raw/headlines.valid', sep = '이런건없겠지', header = None, names = ['text'],engine='python')\n",
    "headlines_test = pd.read_csv(data_dir+'CNN_NYT/raw/headlines.test', sep = '이런건없겠지', header = None, names = ['text'],engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickbait_train, clickbait_valid = train_test_split(clickbait, test_size = 0.2, shuffle = True)\n",
    "clickbait_valid, clickbait_test = train_test_split(clickbait_valid, test_size = 0.5, shuffle = True)\n",
    "humor_train, humor_valid = train_test_split(humor, test_size = 0.2, shuffle = True)\n",
    "humor_valid, humor_test = train_test_split(humor_valid, test_size = 0.5, shuffle = True)\n",
    "romance_train, romance_valid = train_test_split(romance, test_size = 0.2, shuffle = True)\n",
    "romance_valid, romance_test = train_test_split(romance_valid, test_size = 0.5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1341134"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SIZE = len(romance_train) + len(humor_train) + len(clickbait_train) + len(headlines_train)\n",
    "TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[unused1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[unused2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[unused3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[unused4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28991</th>\n",
       "      <td>##）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28992</th>\n",
       "      <td>##，</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28993</th>\n",
       "      <td>##－</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28994</th>\n",
       "      <td>##／</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28995</th>\n",
       "      <td>##：</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28996 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0          [PAD]\n",
       "1      [unused1]\n",
       "2      [unused2]\n",
       "3      [unused3]\n",
       "4      [unused4]\n",
       "...          ...\n",
       "28991        ##）\n",
       "28992        ##，\n",
       "28993        ##－\n",
       "28994        ##／\n",
       "28995        ##：\n",
       "\n",
       "[28996 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT의 단어 집합을 vocabulary.txt에 저장\n",
    "with open('vocabulary.txt', 'w') as f:\n",
    "  for token in tokenizer.vocab.keys():\n",
    "    f.write(token + '\\n')\n",
    "df = pd.read_fwf('vocabulary.txt', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_process = lambda x : tf_bert_sampler(x, style_token = _PLAIN_TOKEN)#'random')\n",
    "hds_train = get_ds_pd(headlines_train['text'], repeat = True, shuffle=True, process = hds_process)\n",
    "hds_valid = get_ds_pd(headlines_valid['text'], process = hds_process)\n",
    "hds_test = get_ds_pd(headlines_test['text'], process = hds_process)\n",
    "\n",
    "humor_process = lambda x : tf_bert_sampler(x, style_token = _HUMOR_TOKEN)\n",
    "humords_train = get_ds_pd(humor_train['text'], repeat = True, shuffle=True, process = humor_process)\n",
    "humords_valid = get_ds_pd(humor_valid['text'], process = humor_process)\n",
    "humords_test = get_ds_pd(humor_test['text'], process = humor_process)\n",
    "\n",
    "romance_process = lambda x : tf_bert_sampler(x, style_token = _ROMANCE_TOKEN)\n",
    "romanceds_train = get_ds_pd(romance_train['text'], repeat = True, shuffle=True, process = romance_process)\n",
    "romanceds_valid = get_ds_pd(romance_valid['text'], process = romance_process)\n",
    "romanceds_test = get_ds_pd(romance_test['text'], process = romance_process)\n",
    "\n",
    "click_process = lambda x : tf_bert_sampler(x, style_token = _CLICK_TOKEN)\n",
    "clickds_train = get_ds_pd(clickbait_train['text'], repeat = True, shuffle=True, process = click_process)\n",
    "clickds_valid = get_ds_pd(clickbait_valid['text'], process = click_process)\n",
    "clickds_test = get_ds_pd(clickbait_test['text'], process = click_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dss = [hds_train, humords_train, romanceds_train, clickds_train]\n",
    "valid_dss = [hds_valid, humords_valid, romanceds_valid, clickds_valid]\n",
    "test_dss = [hds_test, humords_test, romanceds_test, clickds_test]\n",
    "train_ds = tf.data.Dataset.sample_from_datasets(train_dss, weights=[0.25, 0.25, 0.25, 0.25])\n",
    "valid_ds = tf.data.Dataset.sample_from_datasets(valid_dss, weights=[0.25, 0.25, 0.25, 0.25])\n",
    "test_ds = tf.data.Dataset.sample_from_datasets(test_dss, weights=[0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<tf.Tensor: shape=(512, 64), dtype=int64, numpy=\n",
      "array([[    1,   101,  1285, ...,     0,     0,     0],\n",
      "       [    1,   101,  7893, ...,     0,     0,     0],\n",
      "       [    1,   101,  3343, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [    1,   101,  4856, ...,     0,     0,     0],\n",
      "       [    1,   101,  1106, ...,     0,     0,     0],\n",
      "       [    1,   101, 20086, ...,     0,     0,     0]])>, <tf.Tensor: shape=(512, 64), dtype=int64, numpy=\n",
      "array([[  101,  1567,  2995, ...,     0,     0,     0],\n",
      "       [  101,  7893,  1141, ...,     0,     0,     0],\n",
      "       [  101,   187, 13356, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,   170, 16753, ...,     0,     0,     0],\n",
      "       [  101,  1184,  1106, ...,     0,     0,     0],\n",
      "       [  101,   189,  1183, ...,     0,     0,     0]])>), <tf.Tensor: shape=(512, 64), dtype=int64, numpy=\n",
      "array([[ 1567,  2995,   131, ...,     0,     0,     0],\n",
      "       [ 7893,  1141,  8362, ...,     0,     0,     0],\n",
      "       [  187, 13356,  1811, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  170, 16753,  2641, ...,     0,     0,     0],\n",
      "       [ 1184,  1106,  5363, ...,     0,     0,     0],\n",
      "       [  189,  1183,  2528, ...,     0,     0,     0]])>)\n"
     ]
    }
   ],
   "source": [
    "for x in hds_test:\n",
    "    temp = x\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 128)          0           ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.equal (TFOpLambda)     (None, 128)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)           (None, 128)          0           ['tf.math.equal[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.subtract (TFOpLambda)  (None, 128)          0           ['tf.cast[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 1, 128)      0           ['tf.math.subtract[0][0]']       \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " bert_embedding (BertEmbedding)  (None, 128, 768)    22368768    ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.minimum (TFOpLambda)   (None, 128, 128)     0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " att_conv1d (AttConv1D)         (None, 128, 768)     2962176     ['bert_embedding[0][0]',         \n",
      "                                                                  'bert_embedding[0][0]',         \n",
      "                                                                  'bert_embedding[0][0]',         \n",
      "                                                                  'tf.math.minimum[0][0]']        \n",
      "                                                                                                  \n",
      " att_conv1d_1 (AttConv1D)       (None, 128, 768)     2962176     ['att_conv1d[0][0]',             \n",
      "                                                                  'att_conv1d[0][0]',             \n",
      "                                                                  'att_conv1d[0][0]',             \n",
      "                                                                  'tf.math.minimum[0][0]']        \n",
      "                                                                                                  \n",
      " att_conv1d_2 (AttConv1D)       (None, 128, 768)     2962176     ['att_conv1d_1[0][0]',           \n",
      "                                                                  'att_conv1d_1[0][0]',           \n",
      "                                                                  'att_conv1d_1[0][0]',           \n",
      "                                                                  'tf.math.minimum[0][0]']        \n",
      "                                                                                                  \n",
      " att_conv1d_3 (AttConv1D)       (None, 128, 768)     2962176     ['att_conv1d_2[0][0]',           \n",
      "                                                                  'att_conv1d_2[0][0]',           \n",
      "                                                                  'att_conv1d_2[0][0]',           \n",
      "                                                                  'tf.math.minimum[0][0]']        \n",
      "                                                                                                  \n",
      " att_conv1d_4 (AttConv1D)       (None, 128, 768)     2962176     ['att_conv1d_3[0][0]',           \n",
      "                                                                  'att_conv1d_3[0][0]',           \n",
      "                                                                  'att_conv1d_3[0][0]',           \n",
      "                                                                  'tf.math.minimum[0][0]']        \n",
      "                                                                                                  \n",
      " att_conv1d_5 (AttConv1D)       (None, 128, 768)     2962176     ['att_conv1d_4[0][0]',           \n",
      "                                                                  'att_conv1d_4[0][0]',           \n",
      "                                                                  'att_conv1d_4[0][0]',           \n",
      "                                                                  'tf.math.minimum[0][0]']        \n",
      "                                                                                                  \n",
      " att_conv1d_6 (AttConv1D)       (None, 128, 768)     2962176     ['att_conv1d_5[0][0]',           \n",
      "                                                                  'att_conv1d_5[0][0]',           \n",
      "                                                                  'att_conv1d_5[0][0]',           \n",
      "                                                                  'tf.math.minimum[0][0]']        \n",
      "                                                                                                  \n",
      " att_conv1d_7 (AttConv1D)       (None, 128, 768)     2962176     ['att_conv1d_6[0][0]',           \n",
      "                                                                  'att_conv1d_6[0][0]',           \n",
      "                                                                  'att_conv1d_6[0][0]',           \n",
      "                                                                  'tf.math.minimum[0][0]']        \n",
      "                                                                                                  \n",
      " att_conv1d_8 (AttConv1D)       (None, 128, 768)     2962176     ['att_conv1d_7[0][0]',           \n",
      "                                                                  'att_conv1d_7[0][0]',           \n",
      "                                                                  'att_conv1d_7[0][0]',           \n",
      "                                                                  'tf.math.minimum[0][0]']        \n",
      "                                                                                                  \n",
      " att_conv1d_9 (AttConv1D)       (None, 128, 768)     2962176     ['att_conv1d_8[0][0]',           \n",
      "                                                                  'att_conv1d_8[0][0]',           \n",
      "                                                                  'att_conv1d_8[0][0]',           \n",
      "                                                                  'tf.math.minimum[0][0]']        \n",
      "                                                                                                  \n",
      " att_conv1d_10 (AttConv1D)      (None, 128, 768)     2962176     ['att_conv1d_9[0][0]',           \n",
      "                                                                  'att_conv1d_9[0][0]',           \n",
      "                                                                  'att_conv1d_9[0][0]',           \n",
      "                                                                  'tf.math.minimum[0][0]']        \n",
      "                                                                                                  \n",
      " att_conv1d_11 (AttConv1D)      (None, 128, 768)     2962176     ['att_conv1d_10[0][0]',          \n",
      "                                                                  'att_conv1d_10[0][0]',          \n",
      "                                                                  'att_conv1d_10[0][0]',          \n",
      "                                                                  'tf.math.minimum[0][0]']        \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 64, 768)     0           ['att_conv1d_11[0][0]']          \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64, 28996)    22297924    ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 80,212,804\n",
      "Trainable params: 80,212,804\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = AttConv1D_BE_12()\n",
    "\n",
    "    loss = loss_function\n",
    "    count = TRAIN_SIZE // BATCH_SIZE // 4\n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(1e-3, count)\n",
    "    adamw = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate=1e-4)\n",
    "    mac = accuracy_function\n",
    "    model.compile(loss = loss, optimizer = adamw, metrics = [mac])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    SAVE_DIR + 'style_conformer_random_L12_768_fin.h5', monitor=\"val_accuracy_function\", verbose=1, save_best_only=True,\n",
    "    save_weights_only=True, mode=\"max\", save_freq=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 192/2619 [=>............................] - ETA: 9:16 - loss: 7.3083 - accuracy_function: 0.2527"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=100, steps_per_epoch = TRAIN_SIZE // BATCH_SIZE, validation_data = valid_ds, callbacks = [model_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/content/drive/MyDrive/Colab Notebooks/HG/models/style_conformer_random_L12_768_fin.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(tokenizer, model)\n",
    "tester_ds = get_ds_pd(headlines_test['text'], process = hds_process, batch_size = 1, shuffle =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tf-models-official==2.4.0\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "from official.modeling import tf_utils\n",
    "from official import nlp\n",
    "from official.nlp import bert\n",
    "\n",
    "# Load the required submodules\n",
    "import official.nlp.optimization\n",
    "import official.nlp.bert.bert_models\n",
    "import official.nlp.bert.configs\n",
    "import official.nlp.bert.run_classifier\n",
    "import official.nlp.bert.tokenization\n",
    "import official.nlp.data.classifier_data_lib\n",
    "import official.nlp.modeling.losses\n",
    "import official.nlp.modeling.models\n",
    "import official.nlp.modeling.networks\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text  # A dependency of the preprocessing model\n",
    "import tensorflow_addons as tfa\n",
    "from official.nlp import optimization\n",
    "import numpy as np\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/cased_L-12_H-768_A-12/\"#tensorflow/bert_en_cased_L-12_H-768_A-12\n",
    "tf.io.gfile.listdir(gs_folder_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "bert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\n",
    "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\n",
    "\n",
    "bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
    "\n",
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  bert_classifier, bert_encoder = bert.bert_models.classifier_model(\n",
    "    bert_config, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_encoder.layers[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
